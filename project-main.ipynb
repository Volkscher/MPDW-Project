{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "In this notebook, we retrieve the videos and their captions from 2 different datasets, join their information and index them.\n",
    "The notebook covers: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting 10 videos (maybe we should have them all)\n",
    "This section creates a json file of the top 10 videos from the activitynet videos dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o1WPnnvs00I - 23 moments\n",
      "oGwn4NUeoy8 - 23 moments\n",
      "VEDRmPt_-Ms - 20 moments\n",
      "qF3EbR8y8go - 19 moments\n",
      "DLJqhYP-C0k - 18 moments\n",
      "t6f_O8a4sSg - 18 moments\n",
      "6gyD-Mte2ZM - 18 moments\n",
      "jBvGvVw3R-Q - 18 moments\n",
      "PJ72Yl0B1rY - 17 moments\n",
      "QHn9KyE-zZo - 17 moments\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "#[('o1WPnnvs00I', {'duration': 229.86, 'subset': 'training', 'resolution': '640x480',\n",
    "data:list\n",
    "\n",
    "with open('activity_net.v1-3.min.json', 'r') as json_data:\n",
    "    data = json.load(json_data)\n",
    "    \n",
    "    # 'database' is a <key, valu> pair -> <video_id, video_info>\n",
    "    videos = data['database']\n",
    "    \n",
    "    # Sort the list by number of annotations (video moments)\n",
    "    sorted_list = sorted(videos.items(), key= lambda x: len(x[1]['annotations']), reverse = True)\n",
    "\n",
    "    # Select the top 10 videos \n",
    "    top_10_videos = sorted_list[:10]\n",
    "\n",
    "    # Convert the list of tuples to a dictionary before dumping\n",
    "    top_10_dict = {video_id: video_info for video_id, video_info in top_10_videos}\n",
    "\n",
    "    # Check the video id and number of moments of the items in the list\n",
    "    for video_id, video_info in top_10_videos:\n",
    "        print(f\"{video_id} - {len(video_info['annotations'])} moments\")\n",
    "\n",
    "    #print(top_10_dict.keys) # Each key is a video id\n",
    "\n",
    "with open('top10.json', 'w') as file: # Gotta use the full relative path if running on a python notebook\n",
    "    json.dump(top_10_dict, file, indent=2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Metadata Indexing\n",
    "\n",
    "In this section we process the selected videos from our json file.\n",
    "We first start by creating the OpenSearch index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RequestError",
     "evalue": "RequestError(400, 'resource_already_exists_exception', 'index [user13/GrTTB-RiTiyyvhqn2vwULA] already exists')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRequestError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 86\u001b[0m\n\u001b[0;32m     28\u001b[0m index_body \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     29\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msettings\u001b[39m\u001b[38;5;124m\"\u001b[39m:{\n\u001b[0;32m     30\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m:{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m    }\n\u001b[0;32m     83\u001b[0m }\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Create the index with the specified mappings and settings\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_body\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Check if the index creation was successful\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macknowledged\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\rafae\\anaconda3\\envs\\nlp-cv-ir\\lib\\site-packages\\opensearchpy\\client\\utils.py:176\u001b[0m, in \u001b[0;36mquery_params.<locals>._wrapper.<locals>._wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    174\u001b[0m             params[p] \u001b[38;5;241m=\u001b[39m _escape(v)\n\u001b[1;32m--> 176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, params\u001b[38;5;241m=\u001b[39mparams, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rafae\\anaconda3\\envs\\nlp-cv-ir\\lib\\site-packages\\opensearchpy\\client\\indices.py:244\u001b[0m, in \u001b[0;36mIndicesClient.create\u001b[1;34m(self, index, body, params, headers)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m SKIP_IN_PATH:\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmpty value passed for a required argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPUT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_make_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rafae\\anaconda3\\envs\\nlp-cv-ir\\lib\\site-packages\\opensearchpy\\transport.py:457\u001b[0m, in \u001b[0;36mTransport.perform_request\u001b[1;34m(self, method, url, params, body, timeout, ignore, headers)\u001b[0m\n\u001b[0;32m    455\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 457\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;66;03m# connection didn't fail, confirm its live status\u001b[39;00m\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection_pool\u001b[38;5;241m.\u001b[39mmark_live(connection)\n",
      "File \u001b[1;32mc:\\Users\\rafae\\anaconda3\\envs\\nlp-cv-ir\\lib\\site-packages\\opensearchpy\\transport.py:418\u001b[0m, in \u001b[0;36mTransport.perform_request\u001b[1;34m(self, method, url, params, body, timeout, ignore, headers)\u001b[0m\n\u001b[0;32m    415\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_connection()\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 418\u001b[0m     status, headers_response, data \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;66;03m# Lowercase all the header names for consistency in accessing them.\u001b[39;00m\n\u001b[0;32m    429\u001b[0m     headers_response \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    430\u001b[0m         header\u001b[38;5;241m.\u001b[39mlower(): value \u001b[38;5;28;01mfor\u001b[39;00m header, value \u001b[38;5;129;01min\u001b[39;00m headers_response\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    431\u001b[0m     }\n",
      "File \u001b[1;32mc:\\Users\\rafae\\anaconda3\\envs\\nlp-cv-ir\\lib\\site-packages\\opensearchpy\\connection\\http_urllib3.py:308\u001b[0m, in \u001b[0;36mUrllib3HttpConnection.perform_request\u001b[1;34m(self, method, url, params, body, timeout, ignore, headers)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ignore:\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_request_fail(\n\u001b[0;32m    306\u001b[0m         method, full_url, url, orig_body, duration, response\u001b[38;5;241m.\u001b[39mstatus, raw_data\n\u001b[0;32m    307\u001b[0m     )\n\u001b[1;32m--> 308\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraw_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent-type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_request_success(\n\u001b[0;32m    315\u001b[0m     method, full_url, url, orig_body, response\u001b[38;5;241m.\u001b[39mstatus, raw_data, duration\n\u001b[0;32m    316\u001b[0m )\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus, response\u001b[38;5;241m.\u001b[39mheaders, raw_data\n",
      "File \u001b[1;32mc:\\Users\\rafae\\anaconda3\\envs\\nlp-cv-ir\\lib\\site-packages\\opensearchpy\\connection\\base.py:315\u001b[0m, in \u001b[0;36mConnection._raise_error\u001b[1;34m(self, status_code, raw_data, content_type)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    313\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUndecodable raw error response from server: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, err)\n\u001b[1;32m--> 315\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTP_EXCEPTIONS\u001b[38;5;241m.\u001b[39mget(status_code, TransportError)(\n\u001b[0;32m    316\u001b[0m     status_code, error_message, additional_info\n\u001b[0;32m    317\u001b[0m )\n",
      "\u001b[1;31mRequestError\u001b[0m: RequestError(400, 'resource_already_exists_exception', 'index [user13/GrTTB-RiTiyyvhqn2vwULA] already exists')"
     ]
    }
   ],
   "source": [
    "## New Index Mappings for k-nn vectors and embeddings\n",
    "## (embeddings are the means from the words extracted from the captions)\n",
    "\n",
    "from opensearchpy import OpenSearch\n",
    "import requests\n",
    "from opensearchpy import helpers\n",
    "\n",
    "host = 'api.novasearch.org'\n",
    "port = 443\n",
    "\n",
    "user = 'user13' \n",
    "password = 'rumoao+20' \n",
    "index_name = user # We can only have an index with the same name has our user name.\n",
    "\n",
    "# Create the client with SSL/TLS enabled, but hostname verification disabled.\n",
    "client = OpenSearch(\n",
    "    hosts = [{'host': host, 'port': port}],\n",
    "    http_compress = True, # enables gzip compression for request bodies\n",
    "    http_auth = (user, password),\n",
    "    use_ssl = True,\n",
    "    url_prefix = 'opensearch_v2',\n",
    "    verify_certs = False,\n",
    "    ssl_assert_hostname = False,\n",
    "    ssl_show_warn = False\n",
    ")\n",
    "\n",
    "# The fields and how they are searched and how important they are, are defined in the mappings\n",
    "index_body = {\n",
    "   \"settings\":{\n",
    "      \"index\":{\n",
    "         \"number_of_replicas\":0,\n",
    "         \"number_of_shards\":4,\n",
    "         \"refresh_interval\":\"-1\",\n",
    "         \"knn\":\"true\"\n",
    "      }\n",
    "   },\n",
    "   \"mappings\":{\n",
    "       \"dynamic\":      \"strict\", # Prevents accidental addition of new fields to the index. This way indexed documents must match the index mapping.\n",
    "       \"properties\":{\n",
    "         \"video_id\":{\n",
    "            \"type\":\"keyword\"\n",
    "         },\n",
    "         \"title\":{\n",
    "            \"type\":\"text\",\n",
    "            \"analyzer\":\"english\",\n",
    "            \"similarity\":\"BM25\"\n",
    "         },\n",
    "         \"video_path\":{\n",
    "            \"type\":\"text\"\n",
    "         },\n",
    "         \"duration\":{\n",
    "            \"type\":\"float\"\n",
    "         },\n",
    "         \"description\":{  # The description field is a text field of the join from the en_captions field that is an array of strings.\n",
    "            \"type\":\"text\",\n",
    "            \"analyzer\":\"english\",\n",
    "            \"similarity\":\"BM25\"\n",
    "         },\n",
    "        \"description_embedding\":{\n",
    "            \"type\":\"knn_vector\",\n",
    "            \"dimension\": 768,\n",
    "            \"method\":{\n",
    "               \"name\":\"hnsw\",\n",
    "               \"space_type\":\"innerproduct\", # cosinesimil > innerproduct  because the captions are normalized and this provides better semantic similarity\n",
    "               \"engine\":\"faiss\",\n",
    "               \"parameters\":{\n",
    "                  \"ef_construction\":256,\n",
    "                  \"m\":48\n",
    "               }\n",
    "            }\n",
    "        },\n",
    "        \"annotations\": {\n",
    "                \"type\": \"nested\",\n",
    "                \"properties\": {\n",
    "                    \"segment\": {\"type\": \"float\"},\n",
    "                    \"label\": {\"type\": \"text\"},\n",
    "                    \"is_answer\": {\"type\": \"boolean\"},\n",
    "                    \"confidence\": {\"type\": \"float\"}\n",
    "                }\n",
    "        },\n",
    "      }\n",
    "   }\n",
    "}\n",
    "\n",
    "# Create the index with the specified mappings and settings\n",
    "response = client.indices.create(index=index_name, body=index_body)\n",
    "\n",
    "# Check if the index creation was successful\n",
    "if response['acknowledged']:\n",
    "    print(f\"Index '{index_name}' created successfully!\")\n",
    "else:\n",
    "    print(f\"Failed to create index: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc found\n",
      "Updated description for video_id: DLJqhYP-C0k\n",
      "doc found\n",
      "Updated description for video_id: qF3EbR8y8go\n",
      "doc found\n",
      "Updated description for video_id: oGwn4NUeoy8\n",
      "doc found\n",
      "Updated description for video_id: VEDRmPt_-Ms\n",
      "doc found\n",
      "Updated description for video_id: o1WPnnvs00I\n",
      "doc found\n",
      "Updated description for video_id: jBvGvVw3R-Q\n",
      "doc found\n",
      "Updated description for video_id: t6f_O8a4sSg\n",
      "doc found\n",
      "Updated description for video_id: 6gyD-Mte2ZM\n",
      "doc found\n",
      "Updated description for video_id: QHn9KyE-zZo\n",
      "doc found\n",
      "Updated description for video_id: PJ72Yl0B1rY\n",
      "doc found\n",
      "Updated description for video_id: t6f_O8a4sSg\n",
      "doc found\n",
      "Updated description for video_id: 6gyD-Mte2ZM\n",
      "doc found\n",
      "Updated description for video_id: QHn9KyE-zZo\n",
      "doc found\n",
      "Updated description for video_id: PJ72Yl0B1rY\n",
      "{'_index': 'user13', '_id': '0', '_version': 1, 'result': 'created', '_shards': {'total': 1, 'successful': 1, 'failed': 0}, '_seq_no': 0, '_primary_term': 1}\n",
      "{'_index': 'user13', '_id': '1', '_version': 1, 'result': 'created', '_shards': {'total': 1, 'successful': 1, 'failed': 0}, '_seq_no': 0, '_primary_term': 1}\n",
      "{'_index': 'user13', '_id': '2', '_version': 1, 'result': 'created', '_shards': {'total': 1, 'successful': 1, 'failed': 0}, '_seq_no': 1, '_primary_term': 1}\n",
      "{'_index': 'user13', '_id': '3', '_version': 1, 'result': 'created', '_shards': {'total': 1, 'successful': 1, 'failed': 0}, '_seq_no': 2, '_primary_term': 1}\n",
      "{'_index': 'user13', '_id': '4', '_version': 1, 'result': 'created', '_shards': {'total': 1, 'successful': 1, 'failed': 0}, '_seq_no': 0, '_primary_term': 1}\n",
      "{'_index': 'user13', '_id': '5', '_version': 1, 'result': 'created', '_shards': {'total': 1, 'successful': 1, 'failed': 0}, '_seq_no': 0, '_primary_term': 1}\n",
      "{'_index': 'user13', '_id': '6', '_version': 1, 'result': 'created', '_shards': {'total': 1, 'successful': 1, 'failed': 0}, '_seq_no': 3, '_primary_term': 1}\n",
      "{'_index': 'user13', '_id': '7', '_version': 1, 'result': 'created', '_shards': {'total': 1, 'successful': 1, 'failed': 0}, '_seq_no': 1, '_primary_term': 1}\n",
      "{'_index': 'user13', '_id': '8', '_version': 1, 'result': 'created', '_shards': {'total': 1, 'successful': 1, 'failed': 0}, '_seq_no': 1, '_primary_term': 1}\n",
      "{'_index': 'user13', '_id': '9', '_version': 1, 'result': 'created', '_shards': {'total': 1, 'successful': 1, 'failed': 0}, '_seq_no': 4, '_primary_term': 1}\n"
     ]
    }
   ],
   "source": [
    "# Importing the dataset and indexing its data\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset, trust_remote_code=True is needed to load the dataset from the remote repository.\n",
    "dataset = load_dataset('dataset-download.py', trust_remote_code=True) \n",
    "\n",
    "doc_list = []\n",
    "\n",
    "index_number_id = 0 # Index number to use as document ID (0, 1, 2, ...)\n",
    "\n",
    "with open('C:/Git Repositories/MPDW-Project/top10.json', 'r') as data:\n",
    "    data = json.load(data).items()\n",
    "\n",
    "    # Check the video id and number of moments of the items in the list\n",
    "    for video_id, video_info in data:\n",
    "        # Creating the document to be indexed from the video in the dataset\n",
    "        doc = {\n",
    "            'video_id': video_id, # Document ID\n",
    "            'title': video_info['annotations'][0]['label'], # Title\n",
    "            'video_path': video_info['url'], # Video path\n",
    "            'description': \"\",\n",
    "            'duration': video_info['duration'],\n",
    "            \"annotations\": video_info['annotations']\n",
    "        }\n",
    "\n",
    "        doc_list.append(doc)\n",
    "\n",
    "for split in ['train', 'test', 'validation']:\n",
    "    for video in dataset[split]:\n",
    "        # Iterate through the documents in doc_list\n",
    "        for doc in doc_list:\n",
    "            #print(video['video_id'].replace(\"v_\", \"\"))\n",
    "            video['video_id'] = video['video_id'].replace(\"v_\", \"\") # clean the video_key from the captions dataset, it comes with the format v_<key> instead of just <key>\n",
    "\n",
    "            if doc['video_id'] == video['video_id']:  # Check if the video_id matches\n",
    "                print(\"doc found\")\n",
    "                \n",
    "                description = \"\"  # Initialize the description string\n",
    "\n",
    "                # Combine all the captions into the description\n",
    "                for caption in video['en_captions']:\n",
    "                    description += f\" {caption}\"\n",
    "\n",
    "                # Update the document's description\n",
    "                doc['description'] = description\n",
    "\n",
    "\n",
    "# Sending the docs to the opensearch index\n",
    "for doc in doc_list:\n",
    "    response = client.index(index = index_name, id= index_number_id, body = doc)\n",
    "    \n",
    "    print(response)\n",
    "    \n",
    "    index_number_id+= 1\n",
    "\n",
    "# Refresh the index so the docs are searchable (we weren't getting hits because of this...)\n",
    "client.indices.refresh(index = 'user13')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Query Search Types:\n",
    "- text based search\n",
    "- embeddings based search\n",
    "- boolean filters alone (?)\n",
    "- search with boolean filters (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qF3EbR8y8go 1.4283175 Painting  woman is painting in a white paper green leaves in a chinese tree.  a red paint is shown and woman put a stamp on the corner of the paper.  woman is painting a blue ad purple chinese flower.  a red and black flowers are painted on a white paper with very detail for the same woman in a dark room.  woman used some black painting for make details, put the red stamp on the corner and finished the painting with yellow and reddetails on the flowers.\n",
      "oGwn4NUeoy8 0.9529365 Playing congas  A small group of people are seen on a stage getting their instruments ready.  A woman begins playing the drums while another plays piano and the others watch.  The two continue to play their instruments and others on the side watch.\n",
      "PJ72Yl0B1rY 0.664531 Beach soccer  A group of athletes play beach soccer in several different games and locations surrounded by audiences in bleachers and dancing cheerleaders.  A group of soccer players play beach soccer while an audience claps for them and cheerleaders perform routines in between video clips.   The men are shown winning trophies and making soccer goals until a final sponsor, marketing graphic appears.\n"
     ]
    }
   ],
   "source": [
    "# Simple Text Based Search\n",
    "\n",
    "prompt = \"a woman appears\"\n",
    "\n",
    "query_bm25 = {\n",
    "  'size': 20, # Number of max results to return\n",
    "  '_source': ['video_id', 'title', 'description'], # Index Fields to return\n",
    "  'query': {\n",
    "    'multi_match': {\n",
    "      'query': prompt,\n",
    "      'fields': ['description', 'title'] # Index Fields to search\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "response = client.search(\n",
    "    body = query_bm25,\n",
    "    index = 'user13'\n",
    ")\n",
    "\n",
    "#print(response['hits']['hits']) -> how to view the list of hits\n",
    "\n",
    "# Print each hit\n",
    "for hit in response['hits']['hits']:\n",
    "    print(hit['_source']['video_id'], hit['_score'], hit['_source']['title'], hit['_source']['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings Based Search\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-cv-ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
