{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbfb15c5",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f67f7188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rafae\\Desktop\\MPDW-Project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['A young woman is seen standing in a room and leads into her dancing.',\n",
       "  ' The girl dances around the room while the camera captures her movements.',\n",
       "  ' She continues dancing around the room and ends by laying on the floor.'],\n",
       " ['The video starts with a title logo sequence.',\n",
       "  ' A man and woman are in a living room demonstrating exercises.',\n",
       "  ' The woman lays on the ground.',\n",
       "  \" The man starts pointing to different areas of the woman's body as she does an exercise.\",\n",
       "  ' The woman begins to do small sit ups.',\n",
       "  ' The woman ends with a final title logo sequence.'],\n",
       " ['Two people are seen moving around a kitchen quickly performing various tasks and sitting down.',\n",
       "  ' They then wax down a ski in the kitchen while continuing to move around.'],\n",
       " ['We see a hallway with a wooden floor.',\n",
       "  ' A dog in socks walks slowly out onto the floor as a lady films him.',\n",
       "  ' The dog turns around and goes back to the other room.'],\n",
       " ['A woman and a man are sitting on the sidewalk playing music.',\n",
       "  ' People stand next to them and watch them play.',\n",
       "  ' A little boy holding a yellow ball walks by.',\n",
       "  ' A man poses for a picture in front of them.'],\n",
       " ['A young girl is seen sitting in a chair with a person standing next to her.',\n",
       "  ' The person next to her then piercing one ear followed by the other.',\n",
       "  ' The person rubs lotion on the piercings afterwards.'],\n",
       " ['A woman is shown riding a camel past pyramids in Egypt.',\n",
       "  ' The camel walks as the woman leans forward.',\n",
       "  ' And hand covers the lens as the harness is shown.'],\n",
       " ['A child mops the floor of a hallway in a house.',\n",
       "  ' The child sets the mop down and plays with her family member.',\n",
       "  ' The child walks into the bedroom area and continues to mop the floor.'],\n",
       " ['A man is seen kneeling down on the floor speaking to the camera.',\n",
       "  ' The man mixes up various ingredients and begins laying plaster on the floor.',\n",
       "  ' He measures the floor and tiles and cuts out a piece of tile to lay on the floor.',\n",
       "  ' He continues laying tiles on the floor while looking back to speak to the camera.'],\n",
       " ['Two lines of young men are walking side by side down a road.',\n",
       "  'Then one man stands in a field holding a wooden object and begins twisting it.',\n",
       "  'He then bends down and grabs a ball.',\n",
       "  \"After,the ball is placed on the ground and he picks it up and hits it as if he's playing baseball.\",\n",
       "  'The ball is thrown back and he its it again.',\n",
       "  'Shortly after, a field of men are shown and they begin playing a game against one another.',\n",
       "  'There was a penalty and one players attempts to hit the ball into the goal from the side.',\n",
       "  'After,everyone is pictured lying down on the ground as if they are dead but one person begins to sit up but gets hit in the head by the ball and lays back down.',\n",
       "  'Lastly,the screen flashes to a black screen and the words The End are shown.']]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the dataset and indexing its data\n",
    "\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset, trust_remote_code=True is needed to load the dataset from the remote repository.\n",
    "dataset = load_dataset('dataset-download.py', trust_remote_code=True) \n",
    "\n",
    "doc_list = [dataset['train'][:10]] # Load the first 10 examples of the dataset\n",
    "\n",
    "#print(doc_list)  # Print the first example of the dataset\n",
    "\n",
    "doc_list[0]['en_captions']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8824b295",
   "metadata": {},
   "source": [
    "# Transformer Encoder\n",
    "\n",
    "This notebook covers our understanding of the Tranformer Architecture as required in section 2.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0426fdef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'display' from 'IPython.core.display' (c:\\Users\\rafae\\Desktop\\MPDW-Project\\.venv\\Lib\\site-packages\\IPython\\core\\display.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbertviz\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m model_view, head_view\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Get the interactive Tools for Matplotlib\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m#%matplotlib notebook\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m#%matplotlib inline\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\Desktop\\MPDW-Project\\.venv\\Lib\\site-packages\\bertviz\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhead_view\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m head_view\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_view\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m model_view\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\Desktop\\MPDW-Project\\.venv\\Lib\\site-packages\\bertviz\\head_view.py:5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01muuid\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display, HTML, Javascript\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m format_special_chars, format_attention, num_layers\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhead_view\u001b[39m(\n\u001b[32m     11\u001b[39m         attention=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     12\u001b[39m         tokens=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m         html_action=\u001b[33m'\u001b[39m\u001b[33mview\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     24\u001b[39m ):\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'display' from 'IPython.core.display' (c:\\Users\\rafae\\Desktop\\MPDW-Project\\.venv\\Lib\\site-packages\\IPython\\core\\display.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pprint\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n",
    "from bertviz import model_view, head_view\n",
    "\n",
    "\n",
    "# Get the interactive Tools for Matplotlib\n",
    "#%matplotlib notebook\n",
    "#%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34687deb",
   "metadata": {},
   "source": [
    "## Loading the Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6a439d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rafae\\Desktop\\MPDW-Project\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rafae\\.cache\\huggingface\\hub\\models--nboost--pt-bert-base-uncased-msmarco. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_path = 'cross-encoder/ms-marco-MiniLM-L-12-v2'\n",
    "model_path = 'nboost/pt-bert-base-uncased-msmarco'\n",
    "CLS_token = \"[CLS]\"\n",
    "SEP_token = \"[SEP]\"\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "config = AutoConfig.from_pretrained(model_path,  output_hidden_states=True, output_attentions=True)  \n",
    "model = AutoModel.from_pretrained(model_path, output_hidden_states=True)\n",
    "#model.eval() # Set the model to evaluation mode. This is important for models that have dropout layers, as it ensures that they are not used during inference.\n",
    "# After loading the model, you can inspect its architecture. Tipycally, each model is composed by the embedding layer, the self-attention layers and the output layers. The output layer is always task specific. \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee10203f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PCA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m token_index = tokens.index(\u001b[33m\"\u001b[39m\u001b[33mblack\u001b[39m\u001b[33m\"\u001b[39m) + \u001b[32m1\u001b[39m  \u001b[38;5;66;03m# Offset for [CLS]\u001b[39;00m\n\u001b[32m     12\u001b[39m embeddings = [hidden_states[layer][\u001b[32m0\u001b[39m][token_index].numpy() \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(hidden_states))]\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m pca = \u001b[43mPCA\u001b[49m(n_components=\u001b[32m2\u001b[39m)\n\u001b[32m     14\u001b[39m reduced = pca.fit_transform(embeddings)\n\u001b[32m     16\u001b[39m plt.figure(figsize=(\u001b[32m8\u001b[39m, \u001b[32m5\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'PCA' is not defined"
     ]
    }
   ],
   "source": [
    "sentence = \"Black people commit 60% of the crimes while being 16% of the population.\"\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    hidden_states = outputs.hidden_states  # Tuple: (layer_count, batch_size, seq_len, hidden_dim)\n",
    "\n",
    "# Plot PCA-reduced embeddings of the word \"bank\" across layers\n",
    "token_index = tokens.index(\"black\") + 1  # Offset for [CLS]\n",
    "\n",
    "embeddings = [hidden_states[layer][0][token_index].numpy() for layer in range(len(hidden_states))]\n",
    "pca = PCA(n_components=2)\n",
    "reduced = pca.fit_transform(embeddings)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "for i, (x, y) in enumerate(reduced):\n",
    "    plt.scatter(x, y, label=f\"Layer {i}\")\n",
    "    plt.text(x + 0.01, y + 0.01, f\"L{i}\", fontsize=9)\n",
    "plt.title(\"Contextual Embedding Evolution for 'bank'\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d961596",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "attention = outputs.attentions\n",
    "hidden_states = outputs.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80414c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c71f8c",
   "metadata": {},
   "source": [
    "## Visualization of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66752391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "layer = 1\n",
    "\n",
    "rows = 3\n",
    "cols = 4\n",
    "fig, ax_full = plt.subplots(rows, cols)\n",
    "fig.set_figheight(rows*4)\n",
    "fig.set_figwidth(cols*4+3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "\n",
    "layer = 0\n",
    "for r in range(rows):\n",
    "    for c in range(cols):\n",
    "       \n",
    "        ax = ax_full[r,c]\n",
    "        \n",
    "        plt.rcParams.update({'font.size': 10})\n",
    "        current_hidden_state = hidden_states[layer][0].detach().numpy()\n",
    "        \n",
    "        if current_hidden_state.shape[1] == 2:\n",
    "            twodim = current_hidden_state\n",
    "        else:\n",
    "            twodim = PCA().fit_transform(current_hidden_state)[:,:2]\n",
    "\n",
    "        plt.style.use('default') # https://matplotlib.org/3.5.1/gallery/style_sheets/style_sheets_reference.html\n",
    "        im = ax.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
    "        for word, (x,y) in zip(tokens, twodim):\n",
    "            ax.text(x+0.05, y+0.05, word[1:])\n",
    "        \n",
    "        # Show all ticks and label them with the respective list entries\n",
    "        ax.set_title(\"Layer \" + str(layer))\n",
    "            \n",
    "        # Loop over data dimensions and create text annotations.\n",
    "        layer = layer + 1\n",
    "\n",
    "fig.suptitle(\"Visualization of all output embeddings from all layers\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
