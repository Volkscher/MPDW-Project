{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opening an Index\n",
    "Criação de um index + configurações do index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'opensearchpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopensearchpy\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopensearchpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenSearch\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopensearchpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m helpers\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'opensearchpy'"
     ]
    }
   ],
   "source": [
    "from opensearchpy import OpenSearch\n",
    "from opensearchpy import helpers\n",
    "\n",
    "host = 'api.novasearch.org'\n",
    "port = 443\n",
    "\n",
    "user = 'user13' \n",
    "password = 'rumoao+20' \n",
    "index_name = user # We can only have an index with the same name has our user name.\n",
    "\n",
    "# Create the client with SSL/TLS enabled, but hostname verification disabled.\n",
    "client = OpenSearch(\n",
    "    hosts = [{'host': host, 'port': port}],\n",
    "    http_compress = True, # enables gzip compression for request bodies\n",
    "    http_auth = (user, password),\n",
    "    use_ssl = True,\n",
    "    url_prefix = 'opensearch_v2',\n",
    "    verify_certs = False,\n",
    "    ssl_assert_hostname = False,\n",
    "    ssl_show_warn = False\n",
    ")\n",
    "\n",
    "index_body = {\n",
    "   \"settings\":{\n",
    "      \"index\":{\n",
    "         \"number_of_replicas\":0,\n",
    "         \"number_of_shards\":4,\n",
    "         \"refresh_interval\":\"-1\",\n",
    "         \"knn\":\"true\" ## Support for Knn vector data types\n",
    "      }\n",
    "   },\n",
    "   \"mappings\":{\n",
    "       \"dynamic\":      \"strict\",\n",
    "       \"properties\":{\n",
    "         \"title\":{\n",
    "            \"type\":\"text\"\n",
    "         },\n",
    "         \"description\":{\n",
    "            \"type\":\"text\"\n",
    "         },\n",
    "         \"tags\":{\n",
    "            \"type\":\"keyword\"\n",
    "         },\n",
    "         \"json\":{\n",
    "            \"type\":\"flat_object\"\n",
    "         },\n",
    "         \"video_embedding\": {\n",
    "            \"type\": \"dense_vector\",\n",
    "            \"dims\": 512  # Adjust this based on the dimension of your embeddings (e.g., CLIP typically uses 512)\n",
    "        }\n",
    "      }\n",
    "   }\n",
    "}\n",
    "\n",
    "if client.indices.exists(index=index_name):\n",
    "    print(\"Index already exists.\")\n",
    "else:        \n",
    "    response = client.indices.create(index_name, body=index_body)\n",
    "    print('\\nCreating index:')\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Embeddings\n",
    "\n",
    "Extract embeddings from video descriptions or transcripts.\n",
    "Step 1- Use a transformer model to generate embeddings\n",
    "Step 2 - Index a sample video moment\n",
    "\n",
    "Embeddings are a way to represent words, sentences, or other types of data (like images or videos) in a vector space. These vectors are usually real-valued arrays, where each number represents a particular feature or characteristic of the data being represented.\n",
    "Capturing semantic relationships allows us to capture the meaning of the data.\n",
    "Embeddings are generates using models like CLIP. CLIP is a multimodal model that works wih both images and text, learning joint embeddings for both.\n",
    "\n",
    "Embeddings are dense, real-valued vectors that represent data (like text, images, or video moments) in a way that preserves semantic relationships.\n",
    "\n",
    "They allow you to perform tasks like semantic search by measuring the distance or similarity between vectors.\n",
    "\n",
    "In your project, you'll generate video moment embeddings using models like CLIP and store them in OpenSearch for efficient retrieval based on user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
